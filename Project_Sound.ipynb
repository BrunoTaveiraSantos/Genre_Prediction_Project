{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: GenreGenius"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Project is a model that predicts the genre of the music that the user inputs, can be via a file or a recording"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import librosa\n",
    "import math\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow.keras as keras\n",
    "import matplotlib.pyplot as plt\n",
    "import keras_tuner as kt\n",
    "import pickle\n",
    "from scipy import stats\n",
    "import random\n",
    "import sounddevice as sd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction from Sound Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for Dataset and path to save JSON file later\n",
    "DATASET_PATH = \"genres\"\n",
    "JSON_PATH = \"data.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the sample rate to 22050 samples per second\n",
    "SAMPLE_RATE = 22050 #Sample rate refers to the number of samples, or measurements, taken per second to represent an audio signal.\n",
    "\n",
    "# Set the desired duration of the audio track to 30 seconds\n",
    "DURATION = 30\n",
    "\n",
    "# Calculate the total number of samples in the audio track\n",
    "SAMPLES_PER_TRACK = SAMPLE_RATE * DURATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_mfcc(dataset_path, json_path, n_mfcc=13, n_fft=2048, hop_length= 512, num_segments=5):\n",
    "    # dictionary to store data \n",
    "    data= { \n",
    "        \"mapping\": [],\n",
    "        \"mfcc\": [],\n",
    "        \"labels\": [],\n",
    "       }\n",
    "    \n",
    "    num_samples_per_segment = int(SAMPLES_PER_TRACK / num_segments)\n",
    "    expected_num_mfcc_vectors_per_segment = math.ceil(num_samples_per_segment/hop_length)\n",
    "\n",
    "    # loop through all the genres\n",
    "    for i, (dirpath, dirnames, filenames) in enumerate(os.walk(dataset_path)):\n",
    "        #ensure that we are not at the root level\n",
    "        if dirpath is not dataset_path:\n",
    "            #save semantic label \n",
    "            dirpath_components = dirpath.split(\"/7\") # genre/blues => [\"genre\", \"blues\"]\n",
    "            semantic_label = dirpath_components[-1]\n",
    "            data[\"mapping\"].append(semantic_label)\n",
    "            print(\"\\nProcessing {}\".format(semantic_label))\n",
    "\n",
    "            for f in filenames:\n",
    "                #load the audio file\n",
    "                file_path = os.path.join(dirpath, f)\n",
    "                signal, sr = librosa.load(file_path, sr = SAMPLE_RATE)\n",
    "                # PROCESS segments extrating mfccs and storing data \n",
    "                for s in range(num_segments):\n",
    "                    start_sample = num_samples_per_segment * s\n",
    "                    finish_sample = start_sample + num_samples_per_segment\n",
    "\n",
    "                    #store mfcc for segment if it has the expected lenght\n",
    "                    mfcc = librosa.feature.mfcc(y=signal[start_sample:finish_sample], n_mfcc=n_mfcc, n_fft = n_fft, hop_length=hop_length)\n",
    "                    mfcc = mfcc.T\n",
    "                    if len(mfcc) == expected_num_mfcc_vectors_per_segment:\n",
    "                        data[\"mfcc\"].append(mfcc.tolist())\n",
    "                        data[\"labels\"].append(i-1)\n",
    "                        print(\"{}, segment:{}\".format(file_path, s))\n",
    "    with open (JSON_PATH, \"w\") as fp:\n",
    "        json.dump(data, fp, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the features \n",
    "save_mfcc(DATASET_PATH, JSON_PATH, num_segments=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the data\n",
    "def load_data(dataset_path):\n",
    "    with open(dataset_path, \"r\") as fp:\n",
    "        data = json.load(fp)\n",
    "\n",
    "    inputs = np.array(data[\"mfcc\"])\n",
    "    targets = np.array(data[\"labels\"])\n",
    "\n",
    "    return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare the Train,test and validation sets\n",
    "def prepare_datasets(test_size, validation_size):\n",
    "   inputs, target = load_data(JSON_PATH)\n",
    "   X_train, X_test, y_train, y_test = train_test_split(inputs, target, test_size= test_size)\n",
    "   X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size= validation_size)\n",
    "\n",
    "   X_train = X_train[..., np.newaxis]\n",
    "   X_validation = X_validation[..., np.newaxis]\n",
    "   X_test = X_test[..., np.newaxis]\n",
    "\n",
    "   return X_train, X_validation, X_test, y_train, y_validation, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_shape):\n",
    "    #create model \n",
    "    model = keras.Sequential()\n",
    "    #1st conv layer\n",
    "    model.add(keras.layers.Conv2D(32,(3,3), activation=\"relu\", input_shape=input_shape))\n",
    "    model.add(keras.layers.MaxPool2D((3,3), strides=(2,2), padding= \"same\"))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    #2nd conv layer\n",
    "    model.add(keras.layers.Conv2D(32,(3,3), activation=\"relu\", input_shape=input_shape))\n",
    "    model.add(keras.layers.MaxPool2D((3,3), strides=(2,2), padding= \"same\"))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    #3rd conv layer\n",
    "\n",
    "    model.add(keras.layers.Conv2D(32,(2,2), activation=\"relu\", input_shape=input_shape))\n",
    "    model.add(keras.layers.MaxPool2D((2,2), strides=(2,2), padding= \"same\"))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    #flatten the output and feed into dense layer\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dense(64, activation=\"relu\"))\n",
    "    model.add(keras.layers.Dropout(0.3))\n",
    "    #output layer\n",
    "    model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, X, y):\n",
    "    X = X[np.newaxis, ...]\n",
    "    prediction = model.predict(X) # X-> (1, 130,13,1)\n",
    "\n",
    "    #extract index with max value\n",
    "    predict_index = np.argmax(prediction, axis=1) #\n",
    "    print(\"Expected index: {}, Predicted index:  {}\".format(y, predict_index))\n",
    "    return predict_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_validation, X_test, y_train, y_validation, y_test = prepare_datasets(0.25, 0.2)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (X_train.shape[1],X_train.shape[2],X_train.shape[3])\n",
    "model = build_model(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(learning_rate=0.0001)\n",
    "model.compile(optimizer= optimizer,\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, validation_data=(X_validation,y_validation), batch_size=32, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_error, test_accuracy= model.evaluate(X_test, y_test, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy on test set is: {}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_test[50]\n",
    "y = y_test[50]\n",
    "\n",
    "predict(model, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "pickle.dump(model, open('model.pkl', 'wb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_model(hp):\n",
    "    #create model \n",
    "    model = keras.Sequential()\n",
    "    \n",
    "    hp_activation = hp.Choice('activation', values=['relu', 'tanh'])\n",
    "    hp_filters_1 = hp.Int('filters_1', min_value=20, max_value=40, step=2)\n",
    "    hp_filters_2 = hp.Int('filters_1', min_value=20, max_value=40, step=2)\n",
    "    hp_filters_3 = hp.Int('filters_1', min_value=20, max_value=40, step=2)\n",
    "    hp_layer_1 = hp.Int('layer_1', min_value=50, max_value=100, step=2)\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "    #1st conv layer\n",
    "    model.add(keras.layers.Conv2D(hp_filters_1,(3,3), activation=\"relu\", input_shape=(130,13,1)))\n",
    "    model.add(keras.layers.MaxPool2D((3,3), strides=(2,2), padding= \"same\"))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    #2nd conv layer\n",
    "    model.add(keras.layers.Conv2D(hp_filters_2,(3,3), activation=\"relu\", input_shape=input_shape))\n",
    "    model.add(keras.layers.MaxPool2D((3,3), strides=(2,2), padding= \"same\"))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    #3rd conv layer\n",
    "\n",
    "    model.add(keras.layers.Conv2D(hp_filters_3,(2,2), activation=\"relu\", input_shape=input_shape))\n",
    "    model.add(keras.layers.MaxPool2D((2,2), strides=(2,2), padding= \"same\"))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    #flatten the output and feed into dense layer\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dense(hp_layer_1, activation=\"relu\"))\n",
    "    model.add(keras.layers.Dropout(0.3))\n",
    "    #output layer\n",
    "    model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=hp_learning_rate)\n",
    "    model.compile(optimizer= optimizer,\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_shape' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m tuner \u001b[39m=\u001b[39m kt\u001b[39m.\u001b[39;49mHyperband(tune_model,\n\u001b[0;32m      2\u001b[0m                      objective\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mval_accuracy\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m      3\u001b[0m                      max_epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m,\n\u001b[0;32m      4\u001b[0m                      factor\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m,\n\u001b[0;32m      5\u001b[0m                      directory\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mdir\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m      6\u001b[0m                      project_name\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mx\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\Bruno Santos\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras_tuner\\tuners\\hyperband.py:418\u001b[0m, in \u001b[0;36mHyperband.__init__\u001b[1;34m(self, hypermodel, objective, max_epochs, factor, hyperband_iterations, seed, hyperparameters, tune_new_entries, allow_new_entries, max_retries_per_trial, max_consecutive_failed_trials, **kwargs)\u001b[0m\n\u001b[0;32m    391\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[0;32m    392\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    393\u001b[0m     hypermodel\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    404\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    405\u001b[0m ):\n\u001b[0;32m    406\u001b[0m     oracle \u001b[39m=\u001b[39m HyperbandOracle(\n\u001b[0;32m    407\u001b[0m         objective,\n\u001b[0;32m    408\u001b[0m         max_epochs\u001b[39m=\u001b[39mmax_epochs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    416\u001b[0m         max_consecutive_failed_trials\u001b[39m=\u001b[39mmax_consecutive_failed_trials,\n\u001b[0;32m    417\u001b[0m     )\n\u001b[1;32m--> 418\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(oracle\u001b[39m=\u001b[39;49moracle, hypermodel\u001b[39m=\u001b[39;49mhypermodel, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Bruno Santos\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras_tuner\\engine\\tuner.py:113\u001b[0m, in \u001b[0;36mTuner.__init__\u001b[1;34m(self, oracle, hypermodel, max_model_size, optimizer, loss, metrics, distribution_strategy, directory, project_name, logger, tuner_id, overwrite, executions_per_trial, **kwargs)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[39mif\u001b[39;00m hypermodel \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39mrun_trial \u001b[39mis\u001b[39;00m Tuner\u001b[39m.\u001b[39mrun_trial:\n\u001b[0;32m    106\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    107\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mReceived `hypermodel=None`. We only allow not specifying \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    108\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`hypermodel` if the user defines the search space in \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    109\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`Tuner.run_trial()` by subclassing a `Tuner` class without \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    110\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39musing a `HyperModel` instance.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    111\u001b[0m     )\n\u001b[1;32m--> 113\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[0;32m    114\u001b[0m     oracle\u001b[39m=\u001b[39;49moracle,\n\u001b[0;32m    115\u001b[0m     hypermodel\u001b[39m=\u001b[39;49mhypermodel,\n\u001b[0;32m    116\u001b[0m     directory\u001b[39m=\u001b[39;49mdirectory,\n\u001b[0;32m    117\u001b[0m     project_name\u001b[39m=\u001b[39;49mproject_name,\n\u001b[0;32m    118\u001b[0m     logger\u001b[39m=\u001b[39;49mlogger,\n\u001b[0;32m    119\u001b[0m     overwrite\u001b[39m=\u001b[39;49moverwrite,\n\u001b[0;32m    120\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m    121\u001b[0m )\n\u001b[0;32m    123\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_model_size \u001b[39m=\u001b[39m max_model_size\n\u001b[0;32m    124\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer \u001b[39m=\u001b[39m optimizer\n",
      "File \u001b[1;32mc:\\Users\\Bruno Santos\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras_tuner\\engine\\base_tuner.py:133\u001b[0m, in \u001b[0;36mBaseTuner.__init__\u001b[1;34m(self, oracle, hypermodel, directory, project_name, overwrite, **kwargs)\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreload()\n\u001b[0;32m    131\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    132\u001b[0m     \u001b[39m# Only populate initial space if not reloading.\u001b[39;00m\n\u001b[1;32m--> 133\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_populate_initial_space()\n\u001b[0;32m    135\u001b[0m \u001b[39m# Run in distributed mode.\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[39mif\u001b[39;00m dist_utils\u001b[39m.\u001b[39mis_chief_oracle():\n\u001b[0;32m    137\u001b[0m     \u001b[39m# Blocks forever.\u001b[39;00m\n\u001b[0;32m    138\u001b[0m     \u001b[39m# Avoid import at the top, to avoid inconsistent protobuf versions.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Bruno Santos\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras_tuner\\engine\\base_tuner.py:204\u001b[0m, in \u001b[0;36mBaseTuner._populate_initial_space\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhypermodel\u001b[39m.\u001b[39mdeclare_hyperparameters(hp)\n\u001b[0;32m    203\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moracle\u001b[39m.\u001b[39mupdate_space(hp)\n\u001b[1;32m--> 204\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_activate_all_conditions()\n",
      "File \u001b[1;32mc:\\Users\\Bruno Santos\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras_tuner\\engine\\base_tuner.py:161\u001b[0m, in \u001b[0;36mBaseTuner._activate_all_conditions\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    159\u001b[0m hp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moracle\u001b[39m.\u001b[39mget_space()\n\u001b[0;32m    160\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> 161\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhypermodel\u001b[39m.\u001b[39;49mbuild(hp)\n\u001b[0;32m    162\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moracle\u001b[39m.\u001b[39mupdate_space(hp)\n\u001b[0;32m    164\u001b[0m     \u001b[39m# Update the recorded scopes.\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[5], line 16\u001b[0m, in \u001b[0;36mtune_model\u001b[1;34m(hp)\u001b[0m\n\u001b[0;32m     14\u001b[0m model\u001b[39m.\u001b[39madd(keras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mBatchNormalization())\n\u001b[0;32m     15\u001b[0m \u001b[39m#2nd conv layer\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m model\u001b[39m.\u001b[39madd(keras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mConv2D(hp_filters_2,(\u001b[39m3\u001b[39m,\u001b[39m3\u001b[39m), activation\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mrelu\u001b[39m\u001b[39m\"\u001b[39m, input_shape\u001b[39m=\u001b[39minput_shape))\n\u001b[0;32m     17\u001b[0m model\u001b[39m.\u001b[39madd(keras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mMaxPool2D((\u001b[39m3\u001b[39m,\u001b[39m3\u001b[39m), strides\u001b[39m=\u001b[39m(\u001b[39m2\u001b[39m,\u001b[39m2\u001b[39m), padding\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msame\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m     18\u001b[0m model\u001b[39m.\u001b[39madd(keras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mBatchNormalization())\n",
      "\u001b[1;31mNameError\u001b[0m: name 'input_shape' is not defined"
     ]
    }
   ],
   "source": [
    "tuner = kt.Hyperband(tune_model,\n",
    "                     objective='val_accuracy',\n",
    "                     max_epochs=10,\n",
    "                     factor=3,\n",
    "                     directory='dir',\n",
    "                     project_name='x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search(X_train, y_train, epochs=50, validation_split=0.2, callbacks=[stop_early])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tuner.hypermodel.build(best_hps)\n",
    "history = model.fit(X_train, y_train, epochs=50, validation_split=0.2,\n",
    "                    callbacks=[stop_early])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting New input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame\n",
    "#play music\n",
    "def play_sound(file_sample):\n",
    "    pygame.init()\n",
    "    pygame.mixer.init()\n",
    "\n",
    "    pygame.mixer.music.load(file_sample)\n",
    "    start_position = 45  # Starting position in seconds\n",
    "    pygame.mixer.music.play(start=start_position)\n",
    "\n",
    "    duration = 7  # Duration to play in seconds\n",
    "    pygame.time.delay(int(duration * 1000))  # Delay for the specified duration\n",
    "\n",
    "    pygame.mixer.music.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_new_song(file_sample): \n",
    "  # dictionary to store data \n",
    "    data= { \n",
    "        \"mfcc\": []\n",
    "        \n",
    "       }\n",
    "    duration = 60\n",
    "    SAMPLE_RATE = 22050\n",
    "    SAMPLES_PER_TRACK = SAMPLE_RATE * duration\n",
    "    n_fft = 2048 \n",
    "    hop_length = 512\n",
    "    num_samples_per_segment = int(SAMPLES_PER_TRACK / 20)\n",
    "    expected_num_mfcc_vectors_per_segment = math.ceil(num_samples_per_segment/hop_length)\n",
    "    file_path = file_sample\n",
    "    signal, sr = librosa.load(file_path, sr = SAMPLE_RATE,offset=150 ,duration=duration)\n",
    "                # PROCESS segments extrating mfccs and storing data \n",
    "    for s in range(40):\n",
    "        start_sample = num_samples_per_segment * s\n",
    "        finish_sample = start_sample + num_samples_per_segment\n",
    "\n",
    "        #store mfcc for segment if it has the expected lenght\n",
    "        mfcc = librosa.feature.mfcc(y=signal[start_sample:finish_sample], n_mfcc=13, n_fft = 2048, hop_length=512)\n",
    "        mfcc = mfcc.T\n",
    "        if len(mfcc) == expected_num_mfcc_vectors_per_segment:\n",
    "                data[\"mfcc\"].append(mfcc.tolist())\n",
    "                #print(\"{}, segment:{}\".format(file_path, s))\n",
    "\n",
    "    inputs = np.array(data[\"mfcc\"])\n",
    "    inputs = inputs[..., np.newaxis]\n",
    "    \n",
    "    return inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "model = pickle.load(open('model.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_new(model, X):\n",
    "    X = X[np.newaxis, ...]\n",
    "    prediction = model.predict(X) # X-> (1, 130,13,1)\n",
    "\n",
    "    #extract index with max value\n",
    "    predict_index = np.argmax(prediction, axis=1) #\n",
    "    print(\"Predicted index:  {}\".format(predict_index))\n",
    "    return predict_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_numbers(data):\n",
    "    numbers = []\n",
    "    for arr in data:\n",
    "        numbers.append(arr.tolist())\n",
    "    \n",
    "    mode = stats.mode(numbers, keepdims=True)\n",
    "    return mode.mode[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_score(sound_file):\n",
    "    genres_list = [\"Blues\",\"Classical\",\"Country\", \"Disco\", \"Hiphop\", \"Jazz\",\"Metal\",\"Pop\",\"Raggae\",\"Rock\"]\n",
    "    inputs = process_new_song(sound_file)\n",
    "    prediction = []\n",
    "\n",
    "    for i in range(6):\n",
    "        pred = predict_new(model, inputs[i+1])\n",
    "        prediction.append(pred[0])\n",
    "\n",
    "        result = extract_numbers(prediction)\n",
    "    play_sound(sound_file)\n",
    "    return genres_list[result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 32ms/step\n",
      "Predicted index:  [4]\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Predicted index:  [8]\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Predicted index:  [4]\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Predicted index:  [7]\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Predicted index:  [7]\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Predicted index:  [4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hiphop'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#file_path = input(\"Enter the file path:\")\n",
    "#result = final_score(r\"{}\".format(file_path))\n",
    "result = final_score(r\"C:\\Users\\Bruno Santos\\Desktop\\Iron Hack - Semanas\\Final Project\\FINAL\\Full Project\\test\\test\\J. Cole - MIDDLE CHILD (Official Music Video).wav\")\n",
    "result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict a Recorded Sound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_recorded_song():\n",
    "        data= { \n",
    "        \"mfcc\": []\n",
    "        \n",
    "       }\n",
    "        duration = 30\n",
    "        SAMPLE_RATE = 22050\n",
    "        SAMPLES_PER_TRACK = SAMPLE_RATE * duration\n",
    "        n_fft = 2048 \n",
    "        hop_length = 512\n",
    "        num_samples_per_segment = int(SAMPLES_PER_TRACK / 10)\n",
    "        expected_num_mfcc_vectors_per_segment = math.ceil(num_samples_per_segment/hop_length)\n",
    "        print(\"Recording started...\")\n",
    "\n",
    "        # Record audio\n",
    "        audio = sd.rec(int(duration * SAMPLE_RATE), samplerate=SAMPLE_RATE, channels=1, blocking=True)\n",
    "        audio = audio.flatten()\n",
    "        print(\"Recording finished.\")\n",
    "        for s in range(10):\n",
    "                start_sample = num_samples_per_segment * s\n",
    "                finish_sample = start_sample + num_samples_per_segment\n",
    "\n",
    "                #store mfcc for segment if it has the expected lenght\n",
    "                mfcc = librosa.feature.mfcc(y=audio[start_sample:finish_sample], n_mfcc=13, hop_length=512)\n",
    "                mfcc = mfcc.T\n",
    "                if len(mfcc) == expected_num_mfcc_vectors_per_segment:\n",
    "                        data[\"mfcc\"].append(mfcc.tolist())\n",
    "                        #print(\"{}, segment:{}\".format(s))\n",
    "\n",
    "        inputs = np.array(data[\"mfcc\"])\n",
    "        inputs = inputs[..., np.newaxis]\n",
    "    \n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "model = pickle.load(open('model.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_score(model):\n",
    "    genres_list = [\"Blues\",\"Classical\",\"Country\", \"Disco\", \"Hiphop\", \"Jazz\",\"Metal\",\"Pop\",\"Raggae\",\"Rock\"]\n",
    "    inputs = process_recorded_song()\n",
    "    prediction = []\n",
    "    for i in range(7):\n",
    "        pred = predict_new(model, inputs[i])\n",
    "        prediction.append(pred[0])\n",
    "\n",
    "        result = extract_numbers(prediction)\n",
    "    return genres_list[result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording started...\n",
      "Recording finished.\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "Predicted index:  [9]\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "Predicted index:  [4]\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "Predicted index:  [4]\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "Predicted index:  [7]\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "Predicted index:  [4]\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "Predicted index:  [8]\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "Predicted index:  [8]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hiphop'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_score(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "model = pickle.load(open('model.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79/79 [==============================] - 0s 5ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.88      0.92       263\n",
      "           1       0.98      0.93      0.96       276\n",
      "           2       0.78      0.92      0.84       255\n",
      "           3       0.92      0.88      0.90       215\n",
      "           4       0.85      0.91      0.88       246\n",
      "           5       0.86      0.95      0.90       232\n",
      "           6       0.95      0.96      0.96       256\n",
      "           7       0.85      0.96      0.90       251\n",
      "           8       0.94      0.79      0.86       259\n",
      "           9       0.88      0.74      0.80       246\n",
      "\n",
      "    accuracy                           0.89      2499\n",
      "   macro avg       0.90      0.89      0.89      2499\n",
      "weighted avg       0.90      0.89      0.89      2499\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred, axis=1)  \n",
    "y_pred\n",
    " \n",
    "\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bimode"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
